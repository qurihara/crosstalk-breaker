<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">
  <link rel="stylesheet" href="/css/starter-sample.css">
    <meta charset="UTF-8">
    <title>Crosstalk Detector</title>
  </head>
  <body>
    <article id="article">
      <h1 id="title" class="text-center h2">Crosstalk Detector<br></h1>
      <canvas id="canvas" width="600" height="100"></canvas>
      <br>
      <div class="container-b">
        <div class="btn-toolbar">
          <div class="btn-group">
            <button id="start-button5" class="btn btn-outline-dark">Start recording</button>
            <button id="start-button" class="btn btn-outline-dark">Start</button>
            <button id="start-button2" class="btn btn-outline-dark">Start 0-1-many</button>
            <button id="start-button3" class="btn btn-outline-dark">Start 50f</button>
            <button id="start-button4" class="btn btn-outline-dark">Start 50f reg</button>
            <button id="start-button6" class="btn btn-outline-dark">Start fam1/4</button>
            <button id="start-button7" class="btn btn-outline-dark">Start fam1/4 reg</button>
            <button id="start-button8" class="btn btn-outline-dark">Start fam1/4 sig-reg</button>
            <button id="start-button9" class="btn btn-outline-dark">Start fam-id</button>
          </div>
          <div class="btn-group">
            <button id="clear-button" class="btn btn-outline-dark">Stop</button>
          </div>
        </div>
      </div>
      <div class="container-b">
        <div class="output">
          <ul id="console" class="list-unstyled">
            <li></li>
          </ul>
        </div>
      </div>
    </article>
    <script src="https://obniz.io/js/jquery-3.2.1.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
<!--    <script src="https://qurihara.github.io/tfjs-webcam-predct/js/tensorflow.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/eruda/1.4.3/eruda.min.js"></script> -->
<script>

// eruda.init();
var sound = new Audio("https://rawgit.com/Fulox/FullScreenMario-JSON/master/Sounds/Sounds/mp3/Coin.mp3");
var modelname = 'https://qurihara.github.io/crosstalk-breaker/191126_1024fx20_wnoise/';
var classNames = [];
/*
load the class names
*/
async function loadDict() {
    loc = modelname + 'class_names.txt'
    await $.ajax({
        url: loc,
        dataType: 'text',
    }).done(success);
}
/*
load the class names
*/
function success(data) {
    const lst = data.split(/\n/)
    for (var i = 0; i < lst.length - 1; i++) {
        let symbol = lst[i]
        classNames[i] = symbol
    }
}

//-----------------------
// start button event
//-----------------------

var recmode = false;
$("#start-button5").click(function(){
	startWebcam();
    recmode = true;
});

$("#start-button").click(function(){
	modelname = 'https://qurihara.github.io/crosstalk-breaker/191126_1024fx20_wnoise/';
	loadModel(tf.loadLayersModel);
	loadDict();
    bufferSize = 1024;
    fftSize = bufferSize/2;
    predictCount = 20;
	startWebcam();
});

$("#start-button2").click(function(){
	modelname = 'https://qurihara.github.io/crosstalk-breaker/191126_1024fx20_wnoise-0-1-many/model-class/';
	loadModel(tf.loadLayersModel);
	loadDict();
    bufferSize = 1024;
    fftSize = bufferSize/2;
    predictCount = 20;
	startWebcam();
});

$("#start-button3").click(function(){
	modelname = 'https://qurihara.github.io/crosstalk-breaker/191126_1024fx50_wnoise/model-class/';
	loadModel(tf.loadLayersModel);
	loadDict();
    bufferSize = 1024;
    fftSize = bufferSize/2;
    predictCount = 50;
	startWebcam();
});

$("#start-button4").click(function(){
	modelname = 'https://qurihara.github.io/crosstalk-breaker/191126_1024fx50_wnoise/model-reg/';
	loadModel(tf.loadLayersModel);
// 	loadDict();
    classify = false;
    bufferSize = 1024;
    fftSize = bufferSize/2;
    predictCount = 50;
	startWebcam();
});

  $("#start-button6").click(function(){
	modelname = 'https://qurihara.github.io/crosstalk-breaker/191126_1024fx50_family_fft-half/model-class/';
	loadModel(tf.loadLayersModel);
	loadDict();
    bufferSize = 1024;
    fftSize = bufferSize/4;
    predictCount = 50;
    divOffset = 200;
	startWebcam();
});

$("#start-button7").click(function(){
	modelname = 'https://qurihara.github.io/crosstalk-breaker/191126_1024fx50_family_fft-half/model-reg/';
	loadModel(tf.loadLayersModel);
// 	loadDict();
    classify = false;
    bufferSize = 1024;
    fftSize = bufferSize/4;
    predictCount = 50;
    divOffset = 200;
	startWebcam();
});

$("#start-button8").click(function(){
	modelname = 'https://qurihara.github.io/crosstalk-breaker/191127_1024fx50_family_fft-half-reduced-sigmoid/model-reg/';
	loadModel(tf.loadLayersModel);
// 	loadDict();
    classify = false;
    bufferSize = 1024;
    fftSize = bufferSize/4;
    predictCount = 50;
    divOffset = 200;
    regFactor = 2.0;
    warnThre = 1.6;
	startWebcam();
});

  $("#start-button9").click(function(){
	modelname = 'https://qurihara.github.io/crosstalk-breaker/191127_1024fx50_family_fft-half-identity/model-class/';
	loadModel(tf.loadLayersModel);
 	loadDict();
    classify = true;
    bufferSize = 1024;
    fftSize = bufferSize/4;
    predictCount = 50;
    divOffset = 200;
//     regFactor = 2.0;
//     warnThre = 1.6;
	startWebcam();
});



//-----------------------
// load model
//-----------------------

let model;
async function loadModel(loadf) {
    let modelfile = modelname + 'model.json';
	console.log("model loading.. : " + modelfile);
	$("#console").html(`<li>model loading...</li>`);
	// model=await tf.loadModel(modelfile);
// 	model=await tf.loadLayersModel(modelfile);
// 	model=await tf.loadGraphModel(modelfile);
	model=await loadf(modelfile);
	console.log("model loaded.");
	$("#console").html('<li>' + modelname + ' loaded.</li>');
};

//-----------------------
// start webcam
//-----------------------

var audio = {};
var acontext = new AudioContext();
var mediaStream;
function startWebcam() {
    recordingFlg = true;
	console.log("mic start.");
	$("#console").html(`<li>mic start.</li>`);

	// Older browsers might not implement mediaDevices at all, so we set an empty object first
	if (navigator.mediaDevices === undefined) {
		navigator.mediaDevices = {};
	}

	// Some browsers partially implement mediaDevices. We can't just assign an object
	// with getUserMedia as it would overwrite existing properties.
	// Here, we will just add the getUserMedia property if it's missing.
	if (navigator.mediaDevices.getUserMedia === undefined) {
		navigator.mediaDevices.getUserMedia = function(constraints) {

			// First get ahold of the legacy getUserMedia, if present
			var getUserMedia = navigator.webkitGetUserMedia || navigator.mozGetUserMedia;

			// Some browsers just don't implement it - return a rejected promise with an error
			// to keep a consistent interface
			if (!getUserMedia) {
				return Promise.reject(new Error('getUserMedia is not implemented in this browser'));
			}

			// Otherwise, wrap the call to the old navigator.getUserMedia with a Promise
			return new Promise(function(resolve, reject) {
				getUserMedia.call(navigator, constraints, resolve, reject);
			});
		}
	}

	navigator.mediaDevices.getUserMedia({ audio: true, video: false })
	.then(function(stream) {

        // 録音関連
        localMediaStream = stream;
        var scriptProcessor = audioContext.createScriptProcessor(bufferSize, 1, 1);
        localScriptProcessor = scriptProcessor;
        var mediastreamsource = audioContext.createMediaStreamSource(stream);
        mediastreamsource.connect(scriptProcessor);
        scriptProcessor.onaudioprocess = onAudioProcess;
        scriptProcessor.connect(audioContext.destination);

        // 音声解析関連
        audioAnalyser = audioContext.createAnalyser();
        audioAnalyser.fftSize = 2048;
        frequencyData = new Uint8Array(audioAnalyser.frequencyBinCount);
        timeDomainData = new Uint8Array(audioAnalyser.frequencyBinCount);
        mediastreamsource.connect(audioAnalyser);

    })
	.catch(function(err) {
		console.log(err.name + ": " + err.message);
	});

    buf = tf.buffer([predictCount,fftSize]);
}

//-----------------------
// TensorFlow.js method
// predict tensor
//-----------------------

async function predict(tensor){
	let prediction = await model.predict(tensor).data();
    $("#console").empty();
    if (classify == true){
      //classification
      let results = Array.from(prediction)
                  .map(function(p,i){
      return {
          probability: p,
          className: classNames[i]
      };
      }).sort(function(a,b){
          return b.probability-a.probability;
      }).slice(0,classNames.length);//5);

      results.forEach(function(p){
          $("#console").append(`<li>${p.className} : ${p.probability.toFixed(6)}</li>`);
//           console.log(p.className,p.probability.toFixed(6))
      });
    }else{
      //regression
//       console.log(prediction[0]);
      $("#console").append(`<li>${prediction[0]}</li>`);
      if (prediction[0]*regFactor >= warnThre) {
        $("#console").append(`crosstalk!`);
        onfire();
      }
    }

};

onfire = function(){
  console.log("cross talk!!");
  sound.play();
  setTimeout(function(){
    //
  },1000);
}

//-----------------------
// clear button event
//-----------------------

$("#clear-button").click(function clear() {
    endRecording();
// 	location.reload();
});

var buf;
var counter = 0;
// var stck;// = tf.variable(tf.tensor([0]));//tf.zeros([512], tf.float32); //
var get_fft = function(dat){
//   console.log(dat);
  const fft = tf.tidy(() => {
      let offset = tf.scalar(divOffset);
      let fft = tf.signal.stft(tf.tensor1d(dat),bufferSize,bufferSize).abs().div(offset).flatten().slice(0,fftSize);
      //    console.log(fft.shape);
      return fft.dataSync();
  });
  for (var i=0;i<fftSize;i++) buf.set(fft[i],counter,i);
//   if (counter == 0) stck = fft;
//   else stck = tf.concat([stck,fft]);
//   console.log(stck.shape);
  counter++;
  if (counter == predictCount) {
    counter = 0;
    const mat = buf.toTensor().expandDims().expandDims(-1);
//     let mat = stck.reshape([predictCount,fftSize]).expandDims().expandDims(-1);
//     console.log(stck.size)
    predict(mat);
//     stck = null;
    mat.dispose();
  }
}


// 変数定義
var localMediaStream = null;
var localScriptProcessor = null;
var audioContext = new AudioContext();
var audioData = []; // 録音データ
var recordingFlg = false;

var classify = true; //otherwise regression
var bufferSize = 1024;
var fftSize = bufferSize/2;
var predictCount = 20;
var divOffset = 100;
var regFactor = 1.0;
var warnThre = 2.0;

// キャンバス
var canvas = document.getElementById('canvas');
var canvasContext = canvas.getContext('2d');

// 音声解析
var audioAnalyser = null;


// var min=100000;
// var max= -100000;
// 録音バッファ作成（録音中自動で繰り返し呼び出される）
var onAudioProcess = function(e) {
    if (!recordingFlg) return;

    // 音声のバッファを作成
    var input = e.inputBuffer.getChannelData(0);
//     console.log("samplerate : " + e.inputBuffer.sampleRate + " ,length : " + e.inputBuffer.length + " ,duration : " + e.inputBuffer.duration);
    var bufferData = new Float32Array(bufferSize);
//     let flag = false;
    for (var i = 0; i < bufferSize; i++) {
        bufferData[i] = input[i];
//         if (input[i]<min) {
//           min = input[i];
//           flag = true;
//         }
//         if (input[i]>max) {
//           max = input[i];
//           flag = true;
//         }
    }
//     if(flag === true){
//       console.log("min: " + min + ",max: " + max);
//     }

    if (recmode == false){
      get_fft(bufferData);
    }else{
      audioData.push(bufferData);
    }

    // 波形を解析
    analyseVoice();
};

// 解析用処理
var analyseVoice = function() {
    var fsDivN = audioContext.sampleRate / audioAnalyser.fftSize;
    var spectrums = new Uint8Array(audioAnalyser.frequencyBinCount);
    audioAnalyser.getByteFrequencyData(spectrums);
    canvasContext.clearRect(0, 0, canvas.width, canvas.height);

    canvasContext.beginPath();

    for (var i = 0, len = spectrums.length; i < len; i++) {
        //canvasにおさまるように線を描画
        var x = (i / len) * canvas.width;
        var y = (1 - (spectrums[i] / 255)) * canvas.height;
        if (i === 0) {
            canvasContext.moveTo(x, y);
        } else {
            canvasContext.lineTo(x, y);
        }
        var f = Math.floor(i * fsDivN);  // index -> frequency;

        // 500 Hz単位にy軸の線とラベル出力
        if ((f % 500) === 0) {
            var text = (f < 1000) ? (f + ' Hz') : ((f / 1000) + ' kHz');
            // Draw grid (X)
            canvasContext.fillRect(x, 0, 1, canvas.height);
            // Draw text (X)
            canvasContext.fillText(text, x, canvas.height);
        }
    }

    canvasContext.stroke();

    // x軸の線とラベル出力
    var textYs = ['1.00', '0.50', '0.00'];
    for (var i = 0, len = textYs.length; i < len; i++) {
        var text = textYs[i];
        var gy   = (1 - parseFloat(text)) * canvas.height;
        // Draw grid (Y)
        canvasContext.fillRect(0, gy, canvas.width, 1);
        // Draw text (Y)
        canvasContext.fillText(text, 0, gy);
    }
}


// 解析終了
var endRecording = function() {
    recordingFlg = false;

    if(recmode ===true){
//       console.log(audioData);
      let url = exportWAV(audioData);
      console.log(url);
      $("#console").append(`<a href="${url}">Download Audio</a><a>`);
    }
};

var exportWAV = function(audioData) {
          var encodeWAV = function(samples, sampleRate) {
            var buffer = new ArrayBuffer(44 + samples.length * 2);
            var view = new DataView(buffer);

            var writeString = function(view, offset, string) {
              for (var i = 0; i < string.length; i++){
                view.setUint8(offset + i, string.charCodeAt(i));
              }
            };

            var floatTo16BitPCM = function(output, offset, input) {
              for (var i = 0; i < input.length; i++, offset += 2){
                var s = Math.max(-1, Math.min(1, input[i]));
                output.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
              }
            };

            writeString(view, 0, 'RIFF');  // RIFFヘッダ
            view.setUint32(4, 32 + samples.length * 2, true); // これ以降のファイルサイズ
            writeString(view, 8, 'WAVE'); // WAVEヘッダ
            writeString(view, 12, 'fmt '); // fmtチャンク
            view.setUint32(16, 16, true); // fmtチャンクのバイト数
            view.setUint16(20, 1, true); // フォーマットID
            view.setUint16(22, 1, true); // チャンネル数
            view.setUint32(24, sampleRate, true); // サンプリングレート
            view.setUint32(28, sampleRate * 2, true); // データ速度
            view.setUint16(32, 2, true); // ブロックサイズ
            view.setUint16(34, 16, true); // サンプルあたりのビット数
            writeString(view, 36, 'data'); // dataチャンク
            view.setUint32(40, samples.length * 2, true); // 波形データのバイト数
            floatTo16BitPCM(view, 44, samples); // 波形データ

            return view;
          };

          var mergeBuffers = function(audioData) {
            var sampleLength = 0;
            for (var i = 0; i < audioData.length; i++) {
              sampleLength += audioData[i].length;
            }
            var samples = new Float32Array(sampleLength);
            var sampleIdx = 0;
            for (var i = 0; i < audioData.length; i++) {
              for (var j = 0; j < audioData[i].length; j++) {
                samples[sampleIdx] = audioData[i][j];
                sampleIdx++;
              }
            }
            return samples;
          };

          var dataview = encodeWAV(mergeBuffers(audioData), audioContext.sampleRate);
          var audioBlob = new Blob([dataview], { type: 'audio/wav' });

          var myURL = window.URL || window.webkitURL;
          var url = myURL.createObjectURL(audioBlob);
          return url;
};
</script>
  </body>
</html>
